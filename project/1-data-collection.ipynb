{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using Reddit Tools to collect hate speech loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import os \n",
    "\n",
    "# os.mkdir('data')\n",
    "# os.mkdir('data/group_names')\n",
    "# os.mkdir('data/group_data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from webscrapers.reddit.reddit_bot import RedditBuddy\n",
    "\n",
    "bot = RedditBuddy()\n",
    "bot.include_comments = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Sourcing possible hate content/groups.\n",
    "\n",
    "We start by first seeking out which groups are likely to support the dissemination of hate-based content and generating a list of such subreddits. To do this, we'll source posts `r/AgainstHateSubreddits` community, which is dedicated to tracking this kind of content."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bot.time_filter=\"all\"\n",
    "results = bot.recent_submissions(\n",
    "    subreddit='againsthatesubreddits',\n",
    "    limit=5000\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll then try to extract exact subreddit names from the list returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "results['subreddit_mentioned'] = results['submission_title'].apply(lambda x: re.findall(r'(\\br/\\w+\\b|\\br-\\w+\\b)', str(x)))\n",
    "\n",
    "# antisemitic_subreddits = np.unique(sum(results['subreddit_mentioned'].tolist(), []))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results[['submission_flair', 'subreddit_mentioned']].astype(str).value_counts(sort=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results.head(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "results.to_csv('data/group_names/{}.csv'.format(dt.now().date().isoformat().replace('-', '')), index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compiling data collected across multiple weeks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "data_location = '/Volumes/ROY/comp_ling/datasci/RedditHateStudy/processes/data/group_names'\n",
    "\n",
    "files = [os.path.join(data_location, f) for f in os.listdir(data_location) if f.endswith('.csv') and ('all_dates' not in f) and ('._' not in f)]\n",
    "\n",
    "df = [pd.read_csv(f) for f in files]\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "df = df.drop_duplicates(subset=['submission_id'])\n",
    "\n",
    "df['subreddit_mentioned'] = df['subreddit_mentioned'].apply(lambda x: x.lower() if isinstance(x,str) else \"['']\")\n",
    "del_row = []\n",
    "\n",
    "meta_data_cols = [col for col in list(df) if col not in [ 'subreddit_mentioned']]\n",
    "df_ = []\n",
    "for i in df.index:\n",
    "    subs_mentioned = df['subreddit_mentioned'].loc[i].replace('[', \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(', ')\n",
    "    meta_data = df[meta_data_cols].loc[i].to_list()\n",
    "    df_ += [meta_data+[re.sub(r'(r/|r-)', '', sub)] for sub in subs_mentioned]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.array(df_, dtype=object),\n",
    "    columns=meta_data_cols+['subreddit_mentioned']\n",
    ")\n",
    "\n",
    "df.to_csv(os.path.join(data_location, 'all_dates.csv'), index=False, encoding='utf-8')\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts = df[['submission_flair', 'subreddit_mentioned']].value_counts(sort=False)\n",
    "df_counts = [[k[0], k[1], v] for k,v in counts.items()]\n",
    "df_counts = pd.DataFrame(\n",
    "    np.array(df_counts, dtype=object),\n",
    "    columns = ['submission_flair', 'subreddit_mentioned', 'count']\n",
    ")\n",
    "df_counts.sort_values(by=['submission_flair', 'count'],ascending=False).to_csv(os.path.join(data_location, 'all_counts.csv'), index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_counts.sort_values(by=['submission_flair', 'count'],ascending=False).head(1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Pulling data from problematic subreddits\n",
    " \n",
    "Now that we have a list of problematic subreddits, we can actually go through and pull data from them. The following code is confusing, but it will automate the creation of search queries for grabbing hate-based content from Reddit and pulling those results. \n",
    "\n",
    "We'll use our counts for hate-producing subreddits to generate a list of the top 5 hate-producing groups per target of hate speech.\n",
    "\n",
    "We'll then create a query for each target group, per each subreddit producing that kind of content, using the discriminatory lexicon curated by weaponizedword via their API.\n",
    "\n",
    "We'll then search each subreddit using the query for that target group."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "targets = {\n",
    "    'Antisemitism': [\n",
    "        ('malignant_meaning', 'jew'), \n",
    "        ('malignant_meaning', 'Jew'),\n",
    "        ('malignant_meaning', 'Judaism'),\n",
    "    ],\n",
    "    'LGBTQ+ hatred': [\n",
    "        ('malignant_meaning', 'homosexual'), \n",
    "        ('malignant_meaning', 'Homosexual'),\n",
    "        ('malignant_meaning', 'lesbian'),\n",
    "    ], \n",
    "    'Queerphobia': [\n",
    "        ('malignant_meaning', 'homosexual'), \n",
    "        ('malignant_meaning', 'Homosexual'),\n",
    "        ('malignant_meaning', 'lesbian'),\n",
    "    ],\n",
    "    'Islamophobia': [\n",
    "        ('malignant_meaning', 'Muslim'), \n",
    "        ('malignant_meaning', 'Islam'), \n",
    "    ],\n",
    "    'Gender Hatred': [\n",
    "        ('malignant_meaning', 'female'),\n",
    "        ('malignant_meaning', 'women'),\n",
    "        ('malignant_meaning', 'woman'),\n",
    "    ], \n",
    "    'Misogyny': [\n",
    "        ('malignant_meaning', 'female'),\n",
    "        ('malignant_meaning', 'women'),\n",
    "        ('malignant_meaning', 'woman'),\n",
    "    ],\n",
    "    'Xenophobia': [\n",
    "        ('malignant_meaning', 'immigrant'),\n",
    "        ('malignant_meaning', 'immigrants'),\n",
    "    ],\n",
    "    'Transphobia': [\n",
    "        ('malignant_meaning', 'trans'),\n",
    "        ('malignant_meaning', 'Trans'),\n",
    "    ],\n",
    "    'Racism': [\n",
    "        # Anti-Black\n",
    "        ('malignant_meaning', 'black'),\n",
    "        ('malignant_meaning', 'Black'),\n",
    "        ('malignant_meaning', 'african'),\n",
    "        ('malignant_meaning', 'African'),\n",
    "        \n",
    "        # Anti-Asian\n",
    "        ('malignant_meaning', 'Asian'),\n",
    "        ('malignant_meaning', 'asian'),\n",
    "        \n",
    "        # Anti Hispanic/Latine\n",
    "        ('malignant_meaning', 'hispanic'),\n",
    "        ('malignant_meaning', 'Hispanic'),\n",
    "        ('malignant_meaning', 'Mexican'),\n",
    "        ('malignant_meaning', 'mexican'),\n",
    "        ('malignant_meaning', 'Latin'),\n",
    "        ('malignant_meaning', 'latin'),\n",
    "    ],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:28:37.272140Z",
     "start_time": "2024-01-27T18:28:37.269742Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting subreddit names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "path = '/Volumes/ROY/comp_ling/datasci/RedditHateStudy/processes/data'\n",
    "dfg = pd.read_csv(os.path.join(path, 'group_names/all_counts.csv'))\n",
    "\n",
    "target_query = dict()\n",
    "for k in targets.keys():\n",
    "    sub = dfg.loc[\n",
    "        dfg['submission_flair'].isin([k]) \n",
    "        & ~dfg['subreddit_mentioned'].isna()\n",
    "    ].sort_values(by=['count'],ascending=False)\n",
    "    \n",
    "    target_query[k] = {\n",
    "        'subreddits': sub['subreddit_mentioned'].values[5:10].tolist(),\n",
    "        'query': None\n",
    "    }\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:28:38.344244Z",
     "start_time": "2024-01-27T18:28:38.311973Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating queries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from webscrapers.weaponizedword.api import weaponizedword, query_data\n",
    "LOAD = True\n",
    "\n",
    "ww = weaponizedword()\n",
    "\n",
    "if LOAD:\n",
    "    ww.load_search()\n",
    "else:\n",
    "    ww.search(endpoint_name='get_discriminatory')\n",
    "    ww.save_search()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:28:43.531122Z",
     "start_time": "2024-01-27T18:28:42.158561Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for k,v in targets.items():\n",
    "    query = []\n",
    "    for field, search in v:\n",
    "        query += [ww.create_query_from_results(field, search)]\n",
    "    target_query[k]['query'] = ' OR '.join(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:28:46.678977Z",
     "start_time": "2024-01-27T18:28:46.677361Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'queries.json'), 'w', encoding='utf-8') as f:\n",
    "    txt = json.dumps(target_query, indent=4)\n",
    "    f.write(txt)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:28:52.698911Z",
     "start_time": "2024-01-27T18:28:52.658690Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using queries to grab data from subreddits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.7.0 of praw is outdated. Version 7.7.1 was released Tuesday July 11, 2023.\n"
     ]
    }
   ],
   "source": [
    "from webscrapers.reddit.reddit_bot import RedditBuddy\n",
    "\n",
    "bot = RedditBuddy()\n",
    "# bot.submission_limit = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:29:05.766991Z",
     "start_time": "2024-01-27T18:29:05.590702Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "D = pd.DataFrame()\n",
    "\n",
    "D['hate_target'] = [None]\n",
    "D['subreddit'] = [None]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:29:11.642678Z",
     "start_time": "2024-01-27T18:29:11.617630Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not TextIOWrapper",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mqueries.json\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m----> 4\u001B[0m     target_query \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m f\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/json/__init__.py:339\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(s, (\u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mbytearray\u001B[39m)):\n\u001B[0;32m--> 339\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthe JSON object must be str, bytes or bytearray, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    340\u001B[0m                         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n",
      "\u001B[0;31mTypeError\u001B[0m: the JSON object must be str, bytes or bytearray, not TextIOWrapper"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(path, 'queries.json'), 'r', encoding='utf-8') as f:\n",
    "    target_query = json.loads(f)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:26:15.011307Z",
     "start_time": "2024-01-27T18:26:14.769514Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T18:29:16.431566Z",
     "start_time": "2024-01-27T18:29:16.382744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_carlson Antisemitism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fascismreclaimed Antisemitism\n",
      "received 404 HTTP response\n",
      "greentext Antisemitism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:21<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timpool Antisemitism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:05<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicalcompass Antisemitism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socialjusticeinaction LGBTQ+ hatred\n",
      "received 404 HTTP response\n",
      "worldnationalists LGBTQ+ hatred\n",
      "received 404 HTTP response\n",
      "louderwithcrowder LGBTQ+ hatred\n",
      "received 404 HTTP response\n",
      "europeansocialists LGBTQ+ hatred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:24<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucker_carlson LGBTQ+ hatred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:43<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trueanon Queerphobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fingmemes Islamophobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funnymemes Islamophobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 22/24 [00:22<00:02,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received 429 HTTP response\n",
      "hindutvarises Islamophobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:49<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiandankmemes Islamophobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:40<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiarises Islamophobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:25<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theredpill Gender Hatred\n",
      "received 403 HTTP response\n",
      "trueunpopularopinion Gender Hatred\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:04<00:00,  6.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traditionalmuslims Misogyny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "southasianmasculinity Misogyny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:17<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purplepilldebate Misogyny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:08<00:00,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prolife Misogyny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicalcompassmemes Misogyny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:34<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuckercarlson Xenophobia\n",
      "received 403 HTTP response\n",
      "wallstreetsilver Xenophobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wallstreetsilver Transphobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:36<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antihatecommunities Transphobia\n",
      "received 404 HTTP response\n",
      "louderwithcrowder Transphobia\n",
      "received 404 HTTP response\n",
      "transmedical Transphobia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:40<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tumblrinaction Transphobia\n",
      "received 404 HTTP response\n",
      "conservative Racism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:52<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heckoffcommie Racism\n",
      "received 404 HTTP response\n",
      "timpool Racism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:56<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicalcompass Racism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:29<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "louderwithcrowder Racism\n",
      "received 404 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in target_query.items():\n",
    "    \n",
    "    # Query edits after experimentation\n",
    "    if k == 'Antisemitism':\n",
    "        # v['query'] += ' OR \"soros\" OR \"rothschild\" OR \"globalist\" OR \"jew\"'\n",
    "        v['query'] = '\"soros\" OR \"rothschild\" OR \"globalist\" OR \"jew\"'\n",
    "    if k == 'Gender Hatred':\n",
    "        # v['query'] += ' OR \"women\" OR \"woman\"'\n",
    "        v['query'] = '\"women\" OR \"woman\"'\n",
    "    if k == 'Misogyny':\n",
    "        # v['query'] += ' OR \"women\" OR \"woman\"'\n",
    "        v['query'] = '\"women\" OR \"woman\"'\n",
    "    if k == 'Racism':\n",
    "        # v['query'] += ' OR \"black\" OR \"latin\" OR \"african\" OR \"mexican\"'\n",
    "        v['query'] = '\"black\" OR \"latin\" OR \"african\" OR \"mexican\" OR \"asian\" OR \"pacific islander\"'\n",
    "    if k == 'Transphobia':\n",
    "        v['query'] = 'trans OR \"hormone replacement therapy\" OR \"hormone replacement\" OR \"bottom surgery\" OR \"hormone\"'\n",
    "    if k == 'Xenophobia':\n",
    "        v['query'] += ' OR \"immigrant\" OR \"border\"'\n",
    "    \n",
    "    \n",
    "    for subreddit in v['subreddits']:\n",
    "        print(subreddit, k)\n",
    "        # if (subreddit == 'cringetopia') and (k=='Gender Hatred'):\n",
    "        #     scrape = True\n",
    "        if (D['subreddit'].isin([subreddit]) & D['hate_target'].isin([k])).sum() < 1:\n",
    "            try:\n",
    "                data += [\n",
    "                    bot.search( \n",
    "                        subreddit=subreddit,\n",
    "                        searches=[\n",
    "                            v['query']\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "                data[-1]['hate_target'] = k\n",
    "            except Exception as e:\n",
    "                print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T19:32:43.226999Z",
     "start_time": "2024-01-27T18:29:20.450563Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(185251, 13)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = pd.concat(data, ignore_index=True)\n",
    "D = D.drop_duplicates(subset=['comment_id', 'submission_id', 'hate_target'])\n",
    "\n",
    "D.to_csv(\n",
    "    os.path.join(\n",
    "        path, \n",
    "        'group_data/all-groups-{}-2.tsv'.format(\n",
    "            dt.now().date().isoformat().replace('-', '')\n",
    "        )\n",
    "    ), \n",
    "    sep='\\t', \n",
    "    index=False, \n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "D.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T19:32:46.100339Z",
     "start_time": "2024-01-27T19:32:43.218561Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Misogyny         79958\nGender Hatred    35510\nRacism           26199\nAntisemitism     16380\nTransphobia       8955\nIslamophobia      8903\nXenophobia        3524\nQueerphobia       3390\nLGBTQ+ hatred     2432\nName: hate_target, dtype: int64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D['hate_target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T19:32:46.120202Z",
     "start_time": "2024-01-27T19:32:46.112836Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pulling Content from Antisemitic Subreddits [Single example]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from webscrapers.reddit.reddit_bot import RedditBuddy\n",
    "from datetime import datetime as dt\n",
    "\n",
    "bot = RedditBuddy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data, antisemitic_subreddits = [], ['conspiracy']\n",
    "for subreddit in antisemitic_subreddits:\n",
    "    data += [\n",
    "        bot.search(\n",
    "            subreddit=subreddit,\n",
    "            searches=[\n",
    "                '\"soros\" OR \"rothschild\" OR \"globalist\"'\n",
    "            ]\n",
    "            # time_filter='month'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['parent_level'] = data['parent_id'].apply(lambda x: x.split('_')[0])\n",
    "data['parent_id'] = data['parent_id'].apply(lambda x: x.split('_')[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(list(data))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = data['submission_title'].value_counts()\n",
    "print(len(k))\n",
    "k"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_det = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "df, meta_data_cols = [], [col for col in list(data) if col != 'body']\n",
    "for i in data.index:\n",
    "    text = data['body'].loc[i]\n",
    "    if text:\n",
    "        text = re.sub(r'(?<=<QUOTE>)(.*?)(?=<QUOTE>)', '', text)\n",
    "        text = re.sub('<QUOTE>', '', text)\n",
    "        text = text.split('<br>')\n",
    "        text = sum([sent_det.tokenize(t) for t in text], [])\n",
    "        df += [data[meta_data_cols].loc[i].tolist()+[sent] for sent in text]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.array(df, dtype=object),\n",
    "    columns = meta_data_cols + ['body']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sel = df['body'].apply(lambda x: '[removed]' in x)\n",
    "sel.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[~sel].to_csv(\n",
    "    'data/group_data/{}-{}.csv'.format(\n",
    "        '-'.join(antisemitic_subreddits),\n",
    "        dt.now().date().isoformat().replace('-', '')\n",
    "    ), \n",
    "    index=False, encoding='utf-8'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
